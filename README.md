# Hierarchical-Bayesian-Model
Toy problem implementing hierarchical bayesian inference

Here we replicate a toy problem of the Cognitive Science literature 
("Learning Overhypotheses with Hierarchical Bayesian Models" - Kemp et. at)

This is doubling as a way for me to learn about hierarchical bayesian models as well as probabilistic programming, so I'm using Pyro, a 
probabilistic language built on PyTorch.

The high-level set-up is that we want a learner to sample colored balls from n bags, each bag being generated by the same random
process. Given this data, the learner should be able to make inferences as to the distribution of colors within the (n+1)th bag, given
only one draw. The natural model for this problem of guessing the make-up of a bag is a Dirichlet distribution (for two colors this can
be simplified to a Beta distribution), but this does not allow for learning "across" bags. 

In other words, supposing our prior is a uniform Dirichlet distribution, then the best a non-hierarchical Bayesian model can do is either (a) apply the same flat prior to each successive bag, thus learning from scratch with each bag, or (b) use the posterior Dirichlet from each bag as a prior on next bag, which is equivalent to treating all previous draws as having come from one pooled bag, thus missing out on important structure in the data. This failure can best be seen by an environment in which all bags are either uniformly black or uniformly white. Upon seeing a lot of these bags and drawing a black ball from a new bag, it seems rational to assign very high probability to the bag being uniformly black. But the above model cannot
achieve this

## More Hierarchy!

Intuitively what's going on is that we have two very different parameters in mind when we approach this problem - homogeneity within
bag and across the population as a whole. These can be very different (as in the above case), and . Fortunately we can decompose our Dirichlet parameters into a scalar and a vector, the scalar representing the "clumpiness" of colours (the smaller the value, the more likely bags are to be homogenous) and a vector representing the overall population distribution of colors. We want to learn these parameters as they will define a posterior Dirichlet distribution which can better handle new bags. We use a Gamma and a Dirichlet distribution for our scalar and vector parameter respectively (it's a bit confusing that the distribution of one of our parameters of our lower-level Dirichlet is itself a Dirichlet, but it's easy to think of as it defining the population mean of our lower-level Dirichlet, with the scalar determining the variation between bags). 

That's about it for modelling! We set the priors on these as Gamma(1,1) (an exponential with mean 1), and Dir([1,1,1]) (uniform) respectively, and define the data we've seen, define our generative model, condition in on the observed data, and define
our guide function so we can get on with Variational Inference. By declaring pyro.param in the guide, we're specifying which variables to update at each step so that the difference between the latent variables (specified by pyro.sample) is reduced

## Playing around with it

Have a go experimenting with different numbers of colors, different input data etc. One of the coolest results of this kind of model
(to my mind) is that if you train it on entirely all-black and all-white bags, then if you show it a bag and it draws one red ball, it assigns very high probability to the rest of the balls being red! This is a pretty cool "human-like" inference to make since on the face of it - I think it's really cool how Hierarchical models can get more abstract, "structured" learning (Brenden Lake and Joshua Tenenbaum are doing really cool work in this area atm.)

